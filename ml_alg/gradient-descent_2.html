<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ -->
<!-- 启用Chromium高速渲染模式 -->
<meta name="renderer" content="webkit" />
<meta name="force-rendering" content="webkit"/>
<!-- 禁止百度转码 -->
<meta name="applicable-device" content="pc,mobile" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<!-- 禁止识别电话号码 -->
<meta name="format-detection" content="telephone=no" />

<link rel="shortcut icon" href="../favicon_3.ico" />
<link href="../templets/new/style/common_2.css" rel="stylesheet" />
<title>梯度下降求极值</title>
<meta name="description" content="在上一节内容中，我们从数学的角度解析假设函数和损失函数，然而我们最终的目的要得到一个最佳的拟合直线，因此我们就需要将损失函数的偏差值减到最小，我们把寻找极小值的过" />
</head>
<body>
<div id="topbar" class="clearfix">
	<ul id="product-type" class="left">
		<li>
			<a href="../m_biancheng_default_2.html"><span class="iconfont iconfont-home"></span>首页</a>
		</li>
		<li class="active">
			<a href="../sitemap/sitemap_2.html" rel="nofollow"><span class="iconfont iconfont-book"></span>教程</a>
		</li>
		<li>
			<a href="http://vip.biancheng.net/p/vip/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-vip"></span>VIP会员</a>
		</li>
		<li>
			<a href="../fudao_biancheng_default.html" rel="nofollow" target="_blank"><span class="iconfont iconfont-fudao"></span>辅导班</a>
		</li>
		<li>
			<a href="../view/niz69i_5.html" target="_blank"><span class="iconfont iconfont-chip"></span>嵌入式学习路线</a>
		</li>
		<!-- <li>
			<a href="https://www.54benniao.com/c_course/?from=biancheng" target="_blank"><span class="iconfont iconfont-c-course"></span>C语言高级课程</a>
		</li>
		<li>
			<a href="https://www.54benniao.com/java_course/?from=biancheng" target="_blank"><span class="iconfont iconfont-java-course"></span>Java高级课程</a>
		</li>
		<li>
			<a href="http://vip.biancheng.net/p/q2a/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-q2a"></span>一对一答疑</a>
		</li> -->
	</ul>
</div>
<div id="header" class="clearfix">
	<a id="logo" class="left" href="../m_biancheng_default_2.html">
		<img height="26" src="../templets/new/images/logo_2.png" alt="C语言中文网" />
	</a>
	<ul id="nav-main" class="hover-none left clearfix">
		<li class="wap-yes"><a href="../m_biancheng_default_2.html">首页</a></li>
		<li><a href="../c/c_4.html">C语言教程</a></li>
		<li><a href="../cplus/cplus_2.html">C++教程</a></li>
		<li><a href="../python/python_2.html">Python教程</a></li>
		<li><a href="../java/java_2.html">Java教程</a></li>
		<li><a href="../linux_tutorial/linux_tutorial_2.html">Linux入门</a></li>
		<li><a href="../sitemap/sitemap_2.html" title="网站地图">更多&gt;&gt;</a></li>
	</ul>
	<span id="sidebar-toggle" class="toggle-btn" toggle-target="#sidebar">目录 <span class="iconfont"></span></span>

	<a href="http://vip.biancheng.net/?from=topbar" class="user-info iconfont iconfont-user hover-none" target="_blank" rel="nofollow" title="用户中心"></a>
</div>
<div id="main" class="clearfix">
	<div id="sidebar" class="toggle-target">
	<div id="contents">
		<dt><span class="iconfont iconfont-list-vertical" aria-hidden="true"></span><a href="ml_alg_2.html">机器学习算法</a></dt>
		<dd>
        	<span class="channel-num">1</span>
        	<a href="what-is-ai_2.html">什么是人工智能</a>
        </dd>
<dd>
        	<span class="channel-num">2</span>
        	<a href="term_2.html">机器学习常用术语</a>
        </dd>
<dd>
        	<span class="channel-num">3</span>
        	<a href="env_2.html">机器学习环境搭建</a>
        </dd>
<dd>
        	<span class="channel-num">4</span>
        	<a href="linear-regression_2.html">线性回归算法</a>
        </dd>
<dd>
        	<span class="channel-num">5</span>
        	<a href="linear-model_2.html">构建线性模型</a>
        </dd>
<dd>
        	<span class="channel-num">6</span>
        	<a href="hypothesis-loss_2.html">数学解析线性回归</a>
        </dd>
<dd>
        	<span class="channel-num">7</span>
        	<a href="gradient-descent_2.html">梯度下降求极值</a>
        </dd>
<dd>
        	<span class="channel-num">8</span>
        	<a href="sklearn-linear_2.html">sklearn实现线性回归</a>
        </dd>
<dd>
        	<span class="channel-num">9</span>
        	<a href="logistic-regression_2.html">Logistic回归算法</a>
        </dd>
<dd>
        	<span class="channel-num">10</span>
        	<a href="analysis-logistic_2.html">数学解析Logistic算法</a>
        </dd>
<dd>
        	<span class="channel-num">11</span>
        	<a href="sklearn-logistic_2.html">应用Logistic回归算法</a>
        </dd>
<dd>
        	<span class="channel-num">12</span>
        	<a href="knn_2.html">KNN最邻近分类算法</a>
        </dd>
<dd>
        	<span class="channel-num">13</span>
        	<a href="sklearn-knn_2.html">sklearn实现KNN分类算法</a>
        </dd>
<dd>
        	<span class="channel-num">14</span>
        	<a href="bayes-theorem_2.html">理解贝叶斯公式</a>
        </dd>
<dd>
        	<span class="channel-num">15</span>
        	<a href="naive-bayes_2.html">朴素贝叶斯分类算法原理</a>
        </dd>
<dd>
        	<span class="channel-num">16</span>
        	<a href="sklearn-bayes_2.html">朴素贝叶斯算法应用</a>
        </dd>
<dd>
        	<span class="channel-num">17</span>
        	<a href="decision-tree_2.html">决策树算法if-else原理</a>
        </dd>
<dd>
        	<span class="channel-num">18</span>
        	<a href="conditions_2.html">选择决策树判别条件</a>
        </dd>
<dd>
        	<span class="channel-num">19</span>
        	<a href="comentropy_2.html">信息熵是什么</a>
        </dd>
<dd>
        	<span class="channel-num">20</span>
        	<a href="dcision-cut_2.html">决策树算法和剪枝原理</a>
        </dd>
<dd>
        	<span class="channel-num">21</span>
        	<a href="sklearn-decision_2.html">sklearn决策树分类算法</a>
        </dd>
<dd>
        	<span class="channel-num">22</span>
        	<a href="svm_2.html">初识支持向量机SVM分类算法</a>
        </dd>
<dd>
        	<span class="channel-num">23</span>
        	<a href="linear-indivisibility_2.html">SVM解决线性不可分问题</a>
        </dd>
<dd>
        	<span class="channel-num">24</span>
        	<a href="math-svm_2.html">从数学角度理解SVM分类算法</a>
        </dd>
<dd>
        	<span class="channel-num">25</span>
        	<a href="svm-application_2.html">SVM分类算法应用及实现</a>
        </dd>
<dd>
        	<span class="channel-num">26</span>
        	<a href="what-is-kmeans_2.html">什么是K-means聚类算法</a>
        </dd>
<dd>
        	<span class="channel-num">27</span>
        	<a href="kmeans-theory_2.html">K-means聚类算法原理解析</a>
        </dd>
<dd>
        	<span class="channel-num">28</span>
        	<a href="kmeans-application_2.html">K-means聚类算法的应用以及实现</a>
        </dd>
<dd>
        	<span class="channel-num">29</span>
        	<a href="what-is-ann_2.html">人工神经网络是什么</a>
        </dd>
<dd>
        	<span class="channel-num">30</span>
        	<a href="ann-principle_2.html">神经网络分类算法原理详解</a>
        </dd>
<dd>
        	<span class="channel-num">31</span>
        	<a href="ann-application_2.html">神经网络分类算法的应用及其实现</a>
        </dd>
<dd>
        	<span class="channel-num">32</span>
        	<a href="ensemble-learning_2.html">什么是集成学习算法</a>
        </dd>
<dd>
        	<span class="channel-num">33</span>
        	<a href="random-forest_2.html">集成学习应用：随机森林算法</a>
        </dd>

	</div>
</div>
	<div id="article-wrap">
		<div id="article">
			<div class="arc-info">
	<span class="position"><span class="iconfont iconfont-home2"></span> <a href="../m_biancheng_default_2.html">首页</a> &gt; <a href="ml_alg_2.html">机器学习算法</a></span>
</div>

<div id="ggxc-position-bottom" class="ggxc-box"></div>
			<h1>梯度下降求极值</h1>
			<div class="pre-next-page clearfix">&nbsp;</div>
			<div id="ggxc-arctop-pc-1" class="ggxc-box"></div>
			<div id="arc-body">在《<a href="hypothesis-loss_2.html" target="_blank">线性回归：损失函数和假设函数</a>》一节，从数学的角度解释了假设函数和损失函数，我们最终的目的要得到一个最佳的&ldquo;拟合&rdquo;直线，因此就需要将损失函数的偏差值减到最小，我们把寻找极小值的过程称为&ldquo;优化方法&rdquo;，常用的优化方法有很多，比如共轭梯度法、梯度下降法、牛顿法和拟牛顿法。你可能对于上述方法感到陌生，甚至于害怕，其实大可不必，它们只不过应用了一些数学公式而已。<br />
<br />
本节我们重点学习梯度下降法（Gradient Descent），在认识该方法之前，我们先复习一下高中时的数学知识。
<h2>
	导数</h2>
<a href="https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0/579188?fr=aladdin" target="_blank">导数</a>也叫导函数，或者微商，它是微积分中的重要基础概念，从物理学角度来看，导数是研究物体某一时刻的瞬时速度，比如你开车从家 8:00 出发到公司上班，9:00 到到达公司，这一个小时内的平均车速是 80km/h，而途中<code>8:15:30</code>这一时刻的速度，就被称为瞬时速度，此刻的速度可能是 100km/h，也可能是 20km/h。而从几何意义上来讲，你可以把它理解为该函数曲线在一点上的切线斜率。<br />
<br />
导数有其严格的<a href="https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0/579188?fr=aladdin" target="_blank">数学定义</a>，它巧妙的利用了极限的思想，也就是无限趋近于 0 的思想。设函数 y=f(x) 在点 x0 的某个邻域内有定义，当自变量 x 在 x0 处有增量 &Delta;x，(x0+&Delta;x）也在该邻域内时，相应地函数取得增量 &Delta;y=f(x0+&Delta;x)-f(x0)；如果 &Delta;y 与 &Delta;x 之比当 &Delta;x&rarr;0 时极限存在，则称函数 y=f(x) 在点 x0 处可导，并称这个极限为函数 y=f(x) 在点 x0 处的导数记做 ：<br />
<br />
<div style="text-align: center;">
	<img alt="导数定义" src="../uploads/allimg/210902/1F9555328-0_2.png" /></div>
<br />
那么什么样的函数具有导数呢？是不是所有的函数都有导数？当然不是，而且函数也不一定在其所有点上都有导数。如果某函数在某一点导数存在，则称其在这一点可导，否则称为不可导。可导的函数一定连续；不连续的函数一定不可导。<br />
<br />
导数的发明者是伟大的科学家牛顿与布莱尼茨，它是微积分的一个重要的支柱。在机器学习中，我们只需会用前辈科学家们留下来的知识就行了，比如熟悉常见的导函数公式，以下列举了常用的导数公式：<br />
<br />
<div style="text-align: center;">
	<img alt="导数公式" src="../uploads/allimg/210902/1F9553241-1_2.gif" /></div>
<p class="tip-box">
	关于导数的的推断过程详细可参见<a href="https://zhuanlan.zhihu.com/p/137958330" target="_blank">百度百科</a>。</p>
<h2>
	偏导数</h2>
偏导数虽然和导数只有一字之差，但是却相差甚多，从它们的定义来看，偏导数是指对含有两个自变量的函数中的一个自变量求导，也就是说偏导数要求函数必须具备两个自变量。比如拿 z=f(x,y) 举例，如果只有自变量<code>x</code>变化，而自变量<code>y</code>固定（即看作常量），这时它就是<code>x</code>的一元函数，这函数对<code>x</code>的导数，就称为二元函数<code>z</code>对于<code>x</code>的偏导数，记做 f<sub>x</sub>(x,y) 。<br />
<br />
有如下函数 z = x<sup>2</sup> + 3xy + y<sup>2</sup>，分别求 z 对于 x 、y 的偏导数。如下所示：<br />
<pre class="info-box">
fx(x,y) = 2x + 3y # 关于 x 的偏导数
fy(x,y) = 3x + 2y # 关于 y 的偏导数</pre>
当求 x 的偏导时就要把 y 当做常数项来对待，而当求 y 的偏导时就要把 x 当做常数项对待。关于偏导数还会涉及到高阶偏，如果感兴趣的话可以点击<a href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530802.htm" target="_blank">了解一下</a>。
<h2>
	梯度下降</h2>
梯度下降是机器学习中常用的一种优化方法，主要用来解决求极小值的问题，某个函数在某点的梯度指向该函数取得最大值的方向，那么它的反反向自然就是取得最小值的方向。在解决线性回归和 Logistic（逻辑） 回归问题时，梯度下降方法有着广泛的应用。<br />
<br />
梯度是微积分学的术语，它本质上是一个向量，表示函数在某一点处的方向导数上沿着特定的方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大。梯度下降法的计算过程就是沿梯度方向求解极小值，当然你也可以沿梯度上升的方向求解极大值。<br />
<br />
那么如何能够更好的理解&ldquo;梯度下降&rdquo;呢？如果不考虑其他外在因素，其实你可以把它想象成&ldquo;下山&rdquo;的场景，如何从一个高山上以最快的时间走到山脚下呢？其实很简单，以你所在的当前位置为基准，寻找该位置最陡峭的地方，然后沿着此方向向下走，并且每走一段距离，都要寻找当前位置&ldquo;最陡峭的地方&rdquo;，反复采用上述方法，最终就能以最快的时间抵达山脚下。<br />
<br />
在这个下山的过程中，&ldquo;寻找所处位置最陡峭的地方，并沿此位置向下走&rdquo;最为关键，如果把这个做法对应到函数中，就是找到&ldquo;给定点的梯度&rdquo;而梯度的方向就是函数值变化最快的方向。
<div style="text-align: center;">
	<img alt="梯度下降" src="../uploads/allimg/210902/1F9556407-2_2.gif" /><br />
	图1：示意图</div>
<br />
从上述描述中，你可能感觉到平淡无奇，其实每一个词语都蕴含着数学知识，比如&ldquo;以当前所在位置为基准，找到最陡峭的地方&rdquo;从数学角度来讲就是找到所在点的&ldquo;切线&rdquo;方向，也就是对这点&ldquo;求导&rdquo;，然后循着切线轨迹点反复使用此方法，就可以到达极小值点。<br />
<br />
在《<a href="hypothesis-loss_2.html" target="_blank">线性回归：损失函数和假设函数</a>》一节，我们讲解了线性回归的损失函数，而梯度下降作为一种优化方法，其目的是要使得损失值最小。因此&ldquo;梯度下降&rdquo;就需要控制损失函数的<code>w</code>和<code>b</code>参数来找到最小值。比如控制 w 就会得到如下方法：
<p class="info-box">
	w<sub>新</sub>=w<sub>旧</sub> - 学习率 * 损失值</p>
通过梯度下降计算极小值时，需要对损失函数的<code>w</code>求偏导求得，这个偏导也就是&ldquo;梯度&rdquo;，通过损失值来调节<code>w</code>，不断缩小损失值直到最小，这也正是梯度下降的得名来由。<br />
<br />
&ldquo;学习率&rdquo;是一个由外部输入的参数，被称为&ldquo;超参数&rdquo;，可以形象地把它理解为下山时走的&ldquo;步长&rdquo;大小，想要 w 多调整一点，就把学习率调高一点。不过学习率也不是越高越好，过高的学习率可能导致调整幅度过大，导致无法求得真正的最小值。当损失函数取得极小值时，此时的参数值被称为&ldquo;最优参数&rdquo;。因此，在机器学习中最重要的一点就是寻找&ldquo;最优参数&rdquo;。<br />
<br />
梯度下降是个大家族，它有很多成员，比如批量梯度下降（BGD）、随机梯度下降（SGD）、小批量梯度下降（MBGD），其中批量梯度下降是最常用的，相关内容后续会详细介绍。&nbsp;&nbsp;</div>
			<div id="ggxc-weixin-arcbottom">
	<p>关注公众号「<span class="col-green">站长严长生</span>」，在手机上阅读所有教程，随时随地都能学习。内含一款搜索神器，免费下载全网书籍和视频。</p>
	<p style="margin-top:12px; text-align:center;">
		<img src="../templets/new/images/material/qrcode_mp_4.png" alt="公众号二维码" width="160" /><br />
		<span class="col-green">微信扫码关注公众号</span>
	</p>
</div>
			<div class="pre-next-page clearfix">&nbsp;</div>
			<div id="nice-arcs" class="box-bottom">
    <h4>推荐阅读</h4>
    <ul class="clearfix">
<li><a href="../view/niz69i_8.html" title="一套完整的嵌入式开发学习路线（高薪就业版）" target="_blank">一套完整的嵌入式开发学习路线（高薪就业版）</a></li>
<li><a href="../view/tnnfqo_4.html" title="一套课程卖1万，TMD太贵了！" target="_blank">一套课程卖1万，TMD太贵了！</a></li>
<li><a href="../view/unnurw_4.html" title="跑了3000公里，见了一位大佬" target="_blank">跑了3000公里，见了一位大佬</a></li>
<li><a href="../view/2360_2.html" title="Python raise用法（超级详细，看了无师自通）" target="_blank">Python raise用法（超级详细，看了无师自通）</a></li>
<li><a href="../view/vip_7064_2.html" title="Java中的空对象（null）是怎么回事？" target="_blank">Java中的空对象（null）是怎么回事？</a></li>
<li><a href="../view/vip_7326_2.html" title="Go语言使用匿名结构体解析JSON数据" target="_blank">Go语言使用匿名结构体解析JSON数据</a></li>
<li><a href="../view/9859_2.html" title="区块链是什么" target="_blank">区块链是什么</a></li>
<li><a href="../view/x03jd3d_2.html" title="Word插入图片（3种方法）" target="_blank">Word插入图片（3种方法）</a></li>
<li><a href="../view/vtju01e_2.html" title="Vue watch()函数的用法" target="_blank">Vue watch()函数的用法</a></li>
<li><a href="../view/rb37ohv_2.html" title="《现代编译原理(C语言描述)》修订版PDF下载（高清完整版）" target="_blank">《现代编译原理(C语言描述)》修订版PDF下载（高清完整版）</a></li>
</ul>
</div>
		</div>
		
	</div>
</div>
<script type="text/javascript">
// 当前文章ID
window.arcIdRaw = 'a_' + 9239;
window.arcId = "e9d6CKeys+f5yjZTSLssK7UB1a9Sy3JUeesa7XYRwM1bZAf+mLpmx7qMGqw";
window.typeidChain = "427";
</script>
<div id="footer" class="clearfix">
	<div class="info left">
	<p>精美而实用的网站，分享优质编程教程，帮助有志青年。千锤百炼，只为大作；精益求精，处处斟酌；这种教程，看一眼就倾心。</p>
	<p>
		<a href="../view/8066_2.html" target="_blank" rel="nofollow">关于网站</a> <span>|</span>
		<a href="../view/8092_3.html" target="_blank" rel="nofollow">关于站长</a> <span>|</span>
		<a href="../view/8097_2.html" target="_blank" rel="nofollow">如何完成一部教程</a> <span>|</span>
		<a href="../view/9648_2.html" target="_blank" rel="nofollow">公众号</a> <span>|</span>
		<a href="../view/8093_2.html" target="_blank" rel="nofollow">联系我们</a> <span>|</span>
		<a href="https://m.biancheng.net/sitemap/sitemap_2.html" target="_blank" rel="nofollow">网站地图</a>
	</p>
	<p>Copyright ©2012-2022 biancheng.net, <a href="https://beian.miit.gov.cn" target="_blank" rel="nofollow" style="color:#666;">冀ICP备2022013920号</a>, <img height="13" src="https://m.biancheng.net/templets/new/images/gongan_2.png" alt="公安备案图标" /><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=13110202001352" target="_blank" rel="nofollow" style="color:#666;">冀公网安备13110202001352号</a>
	</p>
	</div>
	<img id="logo_bottom" class="right" src="https://m.biancheng.net/templets/new/images/logo_bottom_2.gif" alt="底部Logo" />
	<span id="return-top"><b>↑</b></span>
</div>

<div id="addweixin-widget">
	<p>
		<script type="text/javascript">
			/*var suffix = 'c';
			var thisMin = (new Date()).getMinutes();
			if(thisMin>=40){
				suffix = 'd';
			}else if(thisMin>=20){
				suffix = 'e';
			}else{
				suffix = 'c';
			}
			document.write('<img src="https://m.biancheng.net/templets/new/images/material/qrcode_wx_'%20+%20suffix%20+'.png?v=1.7.07" alt="微信交流群" width="120" /><br />');*/
		</script>
		<img src="https://m.biancheng.net/templets/new/images/material/qrcode_mp_4.png" alt="微信交流群" width="120" />
		<span>关注微信公众号，加入官方交流群。内含一款搜索神器，免费下载全网书籍和视频。</span>
	</p>
	<span id="close-addweixin-widget" class="iconfont iconfont-close"></span>
</div>

<script type="text/javascript">
window.siteId = 4;
window.cmsTemplets = "/templets/new";
window.cmsTempletsVer = "1.7.07";
window.prePageURL = "/ml_alg/hypothesis-loss.html";
window.nextPageURL = "/ml_alg/sklearn-linear.html";
</script>

<script src="https://m.biancheng.net/templets/new/script/jquery1.12.4.min_2.js"></script>
<script src="https://m.biancheng.net/templets/new/script/common_2.js"></script>
<!-- 51la V6 -->
<span style="display: none;">
<script charset="UTF-8" id="LA_COLLECT" src="https://sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id:"KDf6QzBhogyQjall",ck:"KDf6QzBhogyQjall",autoTrack:true})</script>
</span>
<!-- 51la V5 -->
<!-- <span style="display: none;"><script type="text/javascript" src="https://js.users.51.la/21368967.js"></script></span> -->
</body>
</html>