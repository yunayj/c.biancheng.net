<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ -->
<!-- 启用Chromium高速渲染模式 -->
<meta name="renderer" content="webkit" />
<meta name="force-rendering" content="webkit"/>
<!-- 禁止百度转码 -->
<meta name="applicable-device" content="pc,mobile" />
<meta name="MobileOptimized" content="width" />
<meta name="HandheldFriendly" content="true" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<!-- 禁止识别电话号码 -->
<meta name="format-detection" content="telephone=no" />

<link rel="shortcut icon" href="../favicon_3.ico" />
<link href="../templets/new/style/common_2.css" rel="stylesheet" />
<title>Python爬虫入门教程（非常详细）</title>
<meta name="description" content="网络爬虫又被称为网页蜘蛛、网络机器人。 通俗来讲，网络爬虫就是一段程序，可以在网站上获取需要的信息，如文字、视频、图片等。此外，网络爬虫还有一些不常用的名称，如蚂蚁" />
</head>
<body>
<div id="topbar" class="clearfix">
	<ul id="product-type" class="left">
		<li>
			<a href="../m_biancheng_default_2.html"><span class="iconfont iconfont-home"></span>首页</a>
		</li>
		<li class="active">
			<a href="../sitemap/sitemap_2.html" rel="nofollow"><span class="iconfont iconfont-book"></span>教程</a>
		</li>
		<li>
			<a href="http://vip.biancheng.net/p/vip/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-vip"></span>VIP会员</a>
		</li>
		<li>
			<a href="../fudao_biancheng_default.html" rel="nofollow" target="_blank"><span class="iconfont iconfont-fudao"></span>辅导班</a>
		</li>
		<li>
			<a href="niz69i_5.html" target="_blank"><span class="iconfont iconfont-chip"></span>嵌入式学习路线</a>
		</li>
		<!-- <li>
			<a href="https://www.54benniao.com/c_course/?from=biancheng" target="_blank"><span class="iconfont iconfont-c-course"></span>C语言高级课程</a>
		</li>
		<li>
			<a href="https://www.54benniao.com/java_course/?from=biancheng" target="_blank"><span class="iconfont iconfont-java-course"></span>Java高级课程</a>
		</li>
		<li>
			<a href="http://vip.biancheng.net/p/q2a/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-q2a"></span>一对一答疑</a>
		</li> -->
	</ul>
</div>
<div id="header" class="clearfix">
	<a id="logo" class="left" href="../m_biancheng_default_2.html">
		<img height="26" src="../templets/new/images/logo_2.png" alt="C语言中文网" />
	</a>
	<ul id="nav-main" class="hover-none left clearfix">
		<li class="wap-yes"><a href="../m_biancheng_default_2.html">首页</a></li>
		<li><a href="../c/c_4.html">C语言教程</a></li>
		<li><a href="../cplus/cplus_2.html">C++教程</a></li>
		<li><a href="../python/python_2.html">Python教程</a></li>
		<li><a href="../java/java_2.html">Java教程</a></li>
		<li><a href="../linux_tutorial/linux_tutorial_2.html">Linux入门</a></li>
		<li><a href="../sitemap/sitemap_2.html" title="网站地图">更多&gt;&gt;</a></li>
	</ul>
	<a href="http://vip.biancheng.net/?from=topbar" class="user-info iconfont iconfont-user hover-none" target="_blank" rel="nofollow" title="用户中心"></a>
</div>
<div id="main-no-course" class="clearfix">
	<div class="arc-info">
		<span class="position"><span class="iconfont iconfont-home2"></span> <a href="../m_biancheng_default_2.html">首页</a> &gt; 编程笔记</span>
	</div>
	<div id="ggxc-position-bottom" class="ggxc-box"></div>
	<h1>Python爬虫入门教程（非常详细）</h1>
	<div id="ggxc-arctop-pc-1" class="ggxc-box"></div>
	<div id="arc-body">网络爬虫又被称为网页蜘蛛、网络机器人。<br />
<br />
通俗来讲，网络爬虫就是一段程序，可以在网站上获取需要的信息，如文字、视频、图片等。此外，网络爬虫还有一些不常用的名称，如蚂蚁、自动索引、模拟程序或蠕虫等。<br />
<br />
本教程讲解 Python 与网络爬虫，包括爬虫原理与第一个爬虫程序、使用 Python 爬取图片、使用 Scrapy 框架、模拟浏览器等。<br />
<br />
爬虫的设计思路如图1所示：
<ol>
	<li>
		明确需要爬取的网页的URL。</li>
	<li>
		通过HTTP请求来获取对应的网页。</li>
	<li>
		提取网页中的内容，这里有两种情况：
		<ul>
			<li>
				如果是有用的数据，就保存起来；</li>
			<li>
				如果需要继续爬取网页，就重新指定步骤2。</li>
		</ul>
	</li>
</ol>
<br />
<div style="text-align: center;">
	<img alt="爬虫的设计思路" src="../uploads/allimg/230718/1-230GQ62213942_2.png" /><br />
	图1：爬虫的设计思路</div>
<h2 class="center">
	第一个Python爬虫程序</h2>
Python 常使用 requests 模块来模拟浏览器发送请求，获取响应数据。requests 模块属于第三方模块，需要单独安装后才能使用。<br />
<br />
requests 的安装命令如下所示：
<p class="info-box">
	pip install requests</p>
常用的请求方式有两种，分别是 GET 和 POST 请求，对应着 requests.get 和 requests.post 方法。<br />
<br />
它们的语法如下：
<pre class="info-box">
requests.get(url, params=sNone, **kwargs)
requests.post(url, data=None, json=None, **kwargs)</pre>
url 是请求的地址，如果发送 GET 请求，就使用 params 传递参数；如果发送 POST 请求，就使用 data 传递参数。返回值是本次请求对应的响应对象。
<h4>
	范例</h4>
这是第一个 Python 爬虫程序的完整示例。
<pre class="python">
# 导入模块
import requests

# 发送GET请求
r = requests.get(&#39;https://httpbin.org/get&#39;)
print(r.text)

# 发送POST请求，并传递参数
r = requests.get(&#39;https://httpbin.org/get&#39;, params={&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;})
print(r.text)

# 发送POST请求，并传递参数
r = requests.post(&#39;https://httpbin.org/post&#39;, data={&#39;key&#39;: &#39;value&#39;})
print(r.text)

# 其他HTTP请求类型：PUT、DELETE、HEAD和OPTIONS
r = requests.put(&#39;https://httpbin.org/put&#39;, data={&#39;key&#39;: &#39;value&#39;})
print(r.text)

r = requests.delete(&#39;https://httpbin.org/delete&#39;)
print(r.text)

r = requests.head(&#39;https://httpbin.org/get&#39;)
print(r.text)

r = requests.options(&#39;https://httpbin.org/get&#39;)
print(r.text)</pre>
运行结果：
<pre class="info-box">
{
  &quot;args&quot;: {},
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Host&quot;: &quot;httpbin.org&quot;,
    &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot;
  },
  &quot;origin&quot;: &quot;219.156.65.116&quot;,
  &quot;url&quot;: &quot;https://httpbin.org/get&quot;
}
&hellip;&lt;省略部分输出&gt;&hellip;
{
  &quot;args&quot;: {},
  &quot;data&quot;: &quot;&quot;,
  &quot;files&quot;: {},
  &quot;form&quot;: {},
  &quot;headers&quot;: {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,
    &quot;Connection&quot;: &quot;close&quot;,
    &quot;Content-Length&quot;: &quot;0&quot;,
    &quot;Host&quot;: &quot;httpbin.org&quot;,
    &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot;
  },
  &quot;json&quot;: null,
  &quot;origin&quot;: &quot;219.156.65.116&quot;,
  &quot;url&quot;: &quot;https://httpbin.org/delete&quot;
}</pre>
<h4>
	范例分析</h4>
requests 可以很方便地发送各种请求，因为其封装了对应的方法。它常用的是 GET 和 POST 请求，如果需要传递参数，GET 请求使用 params，POST 请求使用 data。<br />
<br />
requests 得到响应对象r，r.text 表示获取的是解码之后的内容。<br />
<br />
如果参数中含有中文，就自动进行 urlencode 编码，不需要使用者再手动实现编码。
<h2 class="center">
	使用Python爬取图片</h2>
上面讲解了爬虫原理与第一个爬虫程序，下面讲解使用 Python 爬取图片。<br />
<br />
首先访问 360 图片网站，如图2所示。<br />
<br />
<div style="text-align: center;">
	<img alt="360 图片网站" src="../uploads/allimg/230718/1-230GQ61512a7_2.png" /><br />
	图2：360 图片网站</div>
<br />
通过观察可以发现，网页中的图片并没有分页，而是可以通过下拉滚动条自动生成下一页。<br />
<br />
通过监听 Network，每次浏览到网页的最后都会多一条请求，仔细观察会发现请求之间是存在一定规律的，如图3所示。<br />
<br />
<div style="text-align: center;">
	<img alt="监听" src="../uploads/allimg/230718/1-230GQ61533230_2.png" /><br />
	图3：监听</div>
<br />
它们都是 http://image.so.com/zj?ch=go&amp;sn={}&amp;listtype=new&amp;temp=1 这样的格式，改变的值只是 sn 中的数字，这就是所需要的页码，可以访问链接进行验证。<br />
<br />
现在已经取得所需要的链接，可写出循环的代码，这里以查找前 10 页为例，代码如下所示。
<pre class="info-box">
for i in range(10):
    url = self.temp_url.format(self.num * 30)</pre>
返回的是 JSON 格式的字符串，转成字典，通过键值对获取图片的 URL，然后向这个 URL 发送请求，获取响应字节。将图片返回的响应字节保存到本地。图片的名字不改变。<br />
<br />
爬取360图片的思路如下：
<ul>
	<li>
		循环准备分页 URL。</li>
	<li>
		分别向分页 URL 发送请求，获取响应 JSON 格式的字符串，提取所有的图片 URL。</li>
	<li>
		分别向图片 URL 发送请求，获取响应字节，保存到本地。</li>
</ul>
<h4>
	范例</h4>
使用 Python 爬取 360 图片的完整代码。
<pre class="python">
from retry import retry
import requests
import json
import time
from fake_useragent import UserAgent


class ImgSpider:
    def _ _init_ _(self):
        &quot;&quot;&quot;初始化参数&quot;&quot;&quot;

        ua = UserAgent()
        # 将要访问的url .{}用于接收参数
        self.temp_url = &quot;http://image.so.com/zj?ch=go&amp;sn={}&amp;listtype=new&amp;temp=1&quot;
        self.headers = {
            &quot;User-Agent&quot;: ua.random,
            &quot;Referer&quot;: &quot;http://s.360.cn/0kee/a.html&quot;,
            &quot;Connection&quot;: &quot;keep-alive&quot;,
        }
        self.num = 0

    def get_img_list(self, url):
        &quot;&quot;&quot;获取存放图片URL的集合&quot;&quot;&quot;

        response = requests.get(url, headers=self.headers)
        html_str = response.content.decode()
        json_str = json.loads(html_str)
        img_str_list = json_str[&quot;list&quot;]
        img_list = []
        for img_object in img_str_list:
            img_list.append(img_object[&quot;qhimg_url&quot;])
        return img_list

    def save_img_list(self, img_list):
        &quot;&quot;&quot;保存图片&quot;&quot;&quot;

        for img in img_list:
            self.save_img(img)
            # time.sleep(2)

    @retry(tries=3)
    def save_img(self, img):
        &quot;&quot;&quot;对获取的图片URL进行下载并将图片保存到本地&quot;&quot;&quot;
        content = requests.get(img).content
        with open(&quot;./data/&quot; + img.split(&quot;/&quot;)[-1], &quot;wb&quot;) as file:
            file.write(content)
        print(str(self.num) + &quot;保存成功&quot;)
        self.num += 1

    def run(self):
        &quot;&quot;&quot;实现主要逻辑&quot;&quot;&quot;

        for i in range(10):
            #获取链接
            url = self.temp_url.format(self.num * 30)
            # 获取数据
            img_list = self.get_img_list(url)
            # 保存数据
            self.save_img_list(img_list)
            break

if _ _name_ _ == &#39;_ _main_ _&#39;:
    img = ImgSpider()
    img.run()</pre>
<h4>
	运行结果</h4>
运行爬虫程序，结果如下所示：
<p class="info-box">
	第0张图片保存成功...<br />
	第1张图片保存成功...<br />
	第2张图片保存成功...<br />
	第3张图片保存成功...<br />
	第4张图片保存成功...<br />
	第5张图片保存成功...<br />
	第6张图片保存成功...<br />
	第7张图片保存成功...<br />
	第8张图片保存成功...<br />
	第9张图片保存成功...<br />
	第10张图片保存成功...<br />
	第11张图片保存成功...<br />
	第12张图片保存成功...<br />
	第13张图片保存成功...<br />
	...&lt;省略以下输出&gt; ...</p>
<br />
查看本地图片，如下所示：
<p class="info-box">
	t0101bc5934a0f24496.jpg&nbsp; t01388041a45aee56e1.jpg&nbsp; t018790c86e27bc4c01.jpg&nbsp; t01aa63d968ee65a5c3.jpg&nbsp; t01d604c9bce2b18c62.jpg<br />
	t0107fb55578a062843.jpg&nbsp; t013b1d241effa05ab6.jpg&nbsp; t0191c8627a98a684a6.jpg&nbsp; t01b4ff750cae438c5a.jpg&nbsp; t01d887dd159577a87e.jpg<br />
	t010909cece5f8e9982.jpg&nbsp; t013ca474bc715ae766.jpg&nbsp; t01931f3fede2c6b03a.jpg&nbsp; t01b52f16508adab4bd.jpg&nbsp; t01d8c656a859bcaf5e.jpg<br />
	t011c4860a95a36bd17.jpg&nbsp; t014cae84604d1faa82.jpg&nbsp; t019710f488f19a2840.jpg&nbsp; t01c1778a8a1c098def.jpg&nbsp; t01d8f3a130704bb822.jpg<br />
	t011ed903a04d9cf633.jpg&nbsp; t014e19b94d67c1a45e.jpg&nbsp; t019830b8a92b05d3a7.jpg&nbsp; t01c3af0dd9ce5fed4f.jpg&nbsp; t01daebb06cc3aa5668.jpg<br />
	...&lt;省略以下输出&gt; ...</p>
<br />
从结果看，数据已经被成功爬取。
<h4>
	范例分析</h4>
分析分页的特点，找到分页的 URL。<br />
<br />
使用 requests 发送请求，获取 JSON 格式的数据，提取图片 URL。<br />
<br />
向图片 URL 发送请求并获取数据，然后将数据保存到本地。
<h2 class="center">
	使用Scrapy框架</h2>
Scrapy 是一个为了爬取网站数据、提取结构性数据而编写的应用框架。<br />
<br />
Scrapy 框架可以应用在数据挖掘、信息处理或存储历史数据等一系列的程序中。其最初是为了页面爬取而设计的，但也可以应用于获取 API 所返回的数据或者通用的网络爬虫。<br />
<br />
Scrapy 是用纯 Python 实现的一个爬取网站数据、提取结构性数据的应用框架，用途广泛。用户只需要定制开发几个模块就能轻松实现一个爬虫程序，用来爬取网页内容、图片等。<br />
<br />
Scrapy 使用 Twisted（其主要对手是 Tornado）异步网络框架来处理网络通信可以加快下载速度，不用自己实现一个框架，并且包含了各种中间件，可以灵活实现各种需求。<br />
<br />
Twisted 是用 Python 实现的基于事件驱动的网络引擎框架，Twisted 支持许多常见的传输及应用层协议，包括 TCP、UDP、SSL/TLS、HTTP、IMAP、SSH、IRC、FTP 等。<br />
<br />
就像 Python 一样，Twisted 也具有&ldquo;内置电池&rdquo;（Batteries-Included）的特点。Twisted 对于其支持的所有协议都带有客户端和服务器的实现，同时附带基于命令行的工具，使得配置和部署产品级的 Twisted 应用框架变得非常方便。
<h3>
	1) 流程介绍</h3>
Scrapy 框架如图4所示图中箭头表示。<br />
<br />
<div style="text-align: center;">
	<img alt="Scrapy 框架" src="../uploads/allimg/230718/1-230GQ616005C_2.png" /><br />
	图4：Scrapy 框架</div>
<br />
各个模块的介绍如下：
<ol>
	<li>
		<h4>
			Scrapy Engine（引擎）</h4>
		负责Spider、Item Pipeline、Downloader、Scheduler之间的通信，以及信号、数据的传递等。</li>
	<li>
		<h4>
			Scheduler（调度器）</h4>
		负责接收发送过来的Request（请求），并按照一定的方式进行整理、排列、入队，当Scrapy Engine需要时，再交还给Scrapy Engine。</li>
	<li>
		<h4>
			Downloader（下载器）</h4>
		负责下载Scrapy Engine发送的所有Request，并将其获取到的Response（回应）交还给Scrapy Engine，由Scrapy Engine交给Spider来处理。</li>
	<li>
		<h4>
			Spider（爬虫）</h4>
		负责处理所有Response，分析并从中提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给Scrapy Engine，再次进入Scheduler。</li>
	<li>
		<h4>
			Item Pipeline（管道文件）</h4>
		负责处理Spider中获取到的Item，并进行后期处理（详细分析、过滤、存储等）。</li>
	<li>
		<h4>
			Downloader Middleware（下载中间件）</h4>
		可以被当作一个能自定义扩展下载功能的控制。</li>
	<li>
		<h4>
			Spider Middleware（爬虫中间件）</h4>
		可以理解为一个可以自定义扩展、操作Scrapy Engine和与Spider通信的功能控件（比如进入Spider的Response和从Spider出去的Request）。</li>
</ol>
<br />
Scrapy 的运作流程如下。<br />
<br />
代码写好后，开始运行程序：
<ol>
	<li>
		Scrapy Engine：Hi！Spider， 你要处理哪一个网站？</li>
	<li>
		Spider：老大要我处理xxxx.com。</li>
	<li>
		Scrapy Engine：你把第一个需要处理的URL给我吧。</li>
	<li>
		Spider：给你，第一个URL是xxxxxxx.com。</li>
	<li>
		Scrapy Engine：Hi！Scheduler，我这儿有Request，你帮我排序入队一下。</li>
	<li>
		Scheduler：好的，正在处理，你等一下。</li>
	<li>
		Scrapy Engine：Hi！Scheduler，把你处理好的Request给我。</li>
	<li>
		Scheduler：给你，这是我处理好的Request。</li>
	<li>
		Scrapy Engine：Hi！Downloader，你按照老大的Downloader Middleware的设置帮我下载一下这个Request。</li>
	<li>
		Downloader：好的！给你，这是下载好的东西。（如果失败：抱歉，这个Request下载失败了。然后Scrapy Engine告诉Scheduler，这个Request下载失败了，你记录一下，我们待会儿再下载。）</li>
	<li>
		Scrapy Engine：Hi！Spider，这是下载好的东西，并且已经按照老大的Downloader Middleware的设置处理过，你自己处理一下（注意！这些Response默认是交给def parse函数处理的）。</li>
	<li>
		Spider：（处理完数据之后对于需要跟进的URL），Hi！Scrapy Engine，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。</li>
	<li>
		Scrapy Engine：Hi ！Item Pipeline，我这儿有个Item你帮我处理一下！Scheduler！这是需要跟进的URL，你帮我处理一下。然后从步骤（4）开始循环，直到获取完老大需要的全部信息。</li>
	<li>
		Item Pipeline/Scheduler：好的，现在就做。</li>
</ol>
<br />
注意，只有当 Scheduler 中不存在任何 Request 时，整个程序才会停止（也就是说，对于下载失败的 URL，Scrapy 也会重新下载。）<br />
<br />
制作 Scrapy 爬虫一共需要以下4步：
<ul>
	<li>
		新建项目（scrapy startproject xxx）：新建一个新的爬虫项目。</li>
	<li>
		明确目标（编写 items.py）：明确想要爬取的目标。</li>
	<li>
		制作爬虫（spiders/xxspider.py）：制作爬虫开始爬取网页。</li>
	<li>
		存储内容（pipelines.py）：设计管道存储爬取内容。</li>
</ul>
<h3>
	2) 安装 Scrapy</h3>
Scrapy的安装命令如下所示。
<p class="info-box">
	pip install scrapy</p>
<h3>
	3) 创建项目</h3>
Scrapy 安装好之后可以开始使用。访问专门供爬虫初学者训练用的网站，如图5所示。<br />
<br />
<div style="text-align: center;">
	<img alt="供爬虫初学者训练用的网站" src="../uploads/allimg/230718/1-230GQ61624a5_2.png" /><br />
	图5：供爬虫初学者训练用的网站</div>
<br />
在该网站中，书籍总共有 1000 本，书籍列表页面一共有 50 页，每页有 20 本书的内容，下面仅爬取所有图书的书名、价格和评级。<br />
<br />
① 首先，要创建一个 Scrapy 项目，在 Shell 中使用如下命令创建项目，如图6所示。
<p class="info-box">
	scrapy startproject spider_01_book</p>
<br />
<div style="text-align: center;">
	<img alt="创建项目" src="../uploads/allimg/230718/1-230GQ61646436_2.png" /><br />
	图6：创建项目</div>
<br />
② 使用 PyCharm 工具打开项目，如图7所示。<br />
<br />
<div style="text-align: center;">
	<img alt="使用 PyCharm 工具打开项目" src="../uploads/allimg/230718/1-230GQ61F5X9_2.png" /><br />
	图7：使用 PyCharm 工具打开项目</div>
<br />
设置项目的 Python 解释器，使用虚拟环境里的 Python 解释器。<br />
<br />
项目中每个文件的说明如下。
<ul>
	<li>
		scrapy.cfg：Scrapy 项目的配置文件，其内定义了项目的配置文件路径、部署相关信息等内容。</li>
	<li>
		items.py：它定义 Item 数据结构，所有的 Item 的定义都可以放在这里。</li>
	<li>
		pipelines.py：它定义 Item Pipeline 的实现，所有的 Item Pipeline 的实现都可以放在这里。</li>
	<li>
		settings.py：它定义项目的全局配置。</li>
	<li>
		middlewares.py：它定义 Spider Middleware 和 Downloader Middleware 的实现。</li>
	<li>
		spiders：其内包含各 Spider 的实现，每个 Spider 都有一个文件。</li>
</ul>
<h3>
	4) 分析页面</h3>
编写爬虫程序之前，首先需要对要爬取的页面进行分析。主流的浏览器中都带有分析页面的工具或插件，这里选用 Chrome 浏览器的开发者工具分析页面。<br />
<br />
单本图书的信息如图8所示。<br />
<div style="text-align: center;">
	<img alt="单本图书的信息" src="../uploads/allimg/230718/1-230GQ61H5153_2.png" /><br />
	图8：单本图书的信息</div>
<br />
在 Chrome 浏览器中访问网站，选中任意一本书，查看其 HTML 代码，如图9所示。<br />
<br />
<div style="text-align: center;">
	<img alt="查看 HTML 代码" src="../uploads/allimg/230718/1-230GQ61JE40_2.png" /><br />
	图9：查看 HTML 代码</div>
<br />
查看后发现，在 &lt;ol class=&quot;row&quot;&gt; 下的 li 中有一个 &lt;article&gt; 标签，这里面存放着该书的所有信息，包括图片、书名、价格和评级：
<ul>
	<li>
		书名为 &lt;a href=&quot;catalogue/a-light-in-the-attic_1000/index.html&quot; alt=&quot;A Light in the Attic&quot;class=&quot;thumbnail&quot;&gt;...&lt;/a&gt; 中的文字；</li>
	<li>
		价格为 &lt;div class=&quot;price_color&quot;&gt;...&lt;/div&gt; 中的文字；</li>
	<li>
		评级为 &lt;p class=&quot;star-rating Three&quot;&gt; 中的 class 的第二个值 Three。</li>
</ul>
<br />
按照数量算是 20 个 &lt;article&gt; 标签，正好和 20 本的数量对应。<br />
<br />
也可以使用 XPath 工具查找，如图10所示。<br />
<br />
<div style="text-align: center;">
	<img alt="使用 XPath 工具查找" src="../uploads/allimg/230718/1-230GQ61PE00_2.png" /><br />
	图10：使用 XPath 工具查找</div>
<br />
选中页面下方的【next】按钮并单击鼠标右键，然后查看其 HTML 代码，如图11所示。<br />
<br />
<div style="text-align: center;">
	<img alt="查看【next】按钮的 HTML 代码" src="../uploads/allimg/230718/1-230GQ61R6323_2.png" /><br />
	图11：查看【next】按钮的 HTML 代码</div>
<br />
在这个被选中的 &lt;a&gt; 标签中，&lt;a href=&quot;catalogue/page-2.html&quot;&gt;next&lt;/a&gt; 中的 href 属性就是要找的 URL，它是一个相对地址，需要拼接 http://books.toscrape.com/ 得到 http://books.toscrape.com/catalogue/page-2.html。<br />
<br />
同样，可以测试一下，改变这里的 page-num 的 num，也就是分页的页码，比如 num 可以为 1~50，表示第 1 页 ~ 第 50 页。
<h3>
	5) 创建爬虫类</h3>
分析完页面后，接下来编写爬虫程序，进入项目并使用如下命令创建爬虫类，如图12所示。
<p class="info-box">
	scrapy startproject spider_01_book</p>
<br />
<div style="text-align: center;">
	<img alt="创建爬虫类" src="../uploads/allimg/230718/1-230GQ61Ta54_2.png" /><br />
	图12：创建爬虫类</div>
<br />
在 PyCharm 中打开项目，在 spiders 包下已经创建好 bookstoscrape.py 文件。在 Scrapy 中编写一个爬虫程序，即实现一个 scrapy.Spider 的子类，代码如下所示。
<pre class="python">
# -*- coding: utf-8 -*-
import scrapy

class BookstoscrapeSpider(scrapy.Spider):
    name = &#39;bookstoscrape&#39;
    allowed_domains = [&#39;books.toscrape.com&#39;]
    start_urls = [&#39;http://books.toscrape.com/&#39;]
　
    def parse(self, response):
        pass</pre>
下面修改 bookstoscrape.py 文件，实现爬取功能，代码如下所示。
<pre class="python">
# -*- coding: utf-8 -*-
import scrapy
　
class BookstoscrapeSpider(scrapy.Spider):
    &quot;&quot;&quot;爬虫类，继承Spider&quot;&quot;&quot;
　
    # 爬虫的名字&mdash;&mdash;每一个爬虫的唯一标识
    name = &#39;bookstoscrape&#39;
    # 允许爬取的域名
    allowed_domains = [&#39;books.toscrape.com&#39;]
    # 初始爬取的URL
    start_urls = [&#39;http://books.toscrape.com/&#39;]
　
    # 解析下载
    def parse(self, response):
        # 提取数据
        # 每一本书的信息在&lt;article class=&quot;product_pod&quot;&gt;中，使用
        # xpath方法找到所有的article 元素，并依次迭代
        for book in response.xpath(&#39;//article[@class=&quot;product_pod&quot;]&#39;):
            # 书名信息在article &gt; h3 &gt; a 元素的title属性里
            # 例如：&lt;a title=&quot;A Light in the Attic&quot;&gt;A Light in the ...&lt;/a&gt;
            name = book.xpath(&#39;./h3/a/@title&#39;).extract_first()
            # 书价信息在article &gt; div[@class=&quot;product_price&quot;] 的文字中
            # 例如：&lt;p class=&quot;price_color&quot;&gt;￡51.77&lt;/p&gt;
            price = book.xpath(&#39;./div[2]/p[1]/text()&#39;).extract_first()[1:]
            # 书的评级在article &gt; p 元素的class属性里
            # 例如 ：&lt;p class=&quot;star-rating Three&quot;&gt;
            rate = book.xpath(&#39;./p/@class&#39;).extract_first().split(&quot; &quot;)[1]
　
            # 返回单个图书对象
            yield {
                &#39;name&#39;: name,
                &#39;price&#39;: price,
                &#39;rate&#39;: rate,
            }
　
        # 提取下一页的URL
        # 下一页的URL在li.next &gt; a 里的href属性值
        # 例如 ：&lt;li class=&quot;next&quot;&gt;&lt;a href=&quot;catalogue/page-2.html&quot;&gt;next&lt;/a&gt;&lt;/li&gt;
        next_url = response.xpath(&#39;//li[@class=&quot;next&quot;]/a/@href&#39;).extract_first()
　
        # 判断
        if next_url:
            # 如果找到下一页的URL，得到绝对路径，构造新的Request对象
            next_url = response.urljoin(next_url)
            # 返回新的Request对象
            yield scrapy.Request(next_url, callback=self.parse)</pre>
如果上述代码中有看不懂的部分，不必担心，这里只要先对实现一个爬虫程序有整体印象即可。<br />
<br />
编写的 spider 对象，必须继承自 scrapy.Spider，要有 name，name 是 spider 的名字，还必须要有 start_urls，这是 Scrapy 下载的第一个网页，告诉 Scrapy 爬取工作从这里开始。parse 函数是 Scrapy 默认调用的，它实现爬取逻辑。<br />
<br />
下面对 BookstoscrapeSpider 的实现进行简单说明，如表1所示。<br />
<br />
<table>
	<caption>
		表1：BookstoscrapeSpider 的实现的说明</caption>
	<tbody>
		<tr>
			<th>
				编号</th>
			<th>
				属性</th>
			<th>
				描述</th>
		</tr>
		<tr>
			<td>
				1</td>
			<td>
				name</td>
			<td>
				一个 Scrapy 项目中可能有多个爬虫，每个爬虫的 name 属性是其自身的唯一标识，在一个项目中不能有同名的爬虫，本例中的爬虫取名为 book stoscrape。</td>
		</tr>
		<tr>
			<td>
				2</td>
			<td>
				allowed_ domains</td>
			<td>
				可选。包含了 Spider 允许爬取的域名列表。当 OffsiteMiddleware 启用时，域名不在列表中的 URL 不会被跟进。</td>
		</tr>
		<tr>
			<td>
				3</td>
			<td>
				start urls</td>
			<td>
				一个爬虫总要从某个（或某些）页面开始爬取，这样的页面称为起始爬取点，start_urls 属性用来设置一个爬虫的起始爬取点。</td>
		</tr>
		<tr>
			<td>
				4</td>
			<td>
				parse(response)</td>
			<td>
				当 response 没有指定回调函数时，该方法是 Scrapy 处理下载的 response 的默认方法。<br />
				<br />
				parse 负责处理 response 并返回处理的数据及（或）跟进的 URL。Spider 对其他的 Request 的回调函数也有相同的要求。<br />
				<br />
				parse 方法及其他的 Request 回调函数必须返回一个包含 Request 及（或）ltem 的可迭代的对象，它的参数为 response ( Response对象）&mdash;&mdash;用于分析的 response。</td>
		</tr>
	</tbody>
</table>
<h3>
	6) 运行爬虫</h3>
写完代码后，运行爬虫爬取数据。在 Shell 中执行 scrapy crawl &lt;SPIDER_NAME&gt; 命令运行爬虫 bookstoscrape ，并将爬取的数据存储到一个 CSV 文件中，如下所示。
<p class="info-box">
	scrapy crawl bookstoscrape -o bookstoscrape.csv</p>
细节说明：
<ul>
	<li>
		crawl 表示启动爬虫。</li>
	<li>
		bookstoscrape 是之前在 bookstoscrape.py 中的 BookstoscrapeSpider 中定义的 name。</li>
	<li>
		-o 表示保存文件的路径，没有这个参数也能启动爬虫，只不过数据没有保存下来而已。</li>
	<li>
		bookstoscrape.csv 是文件名。</li>
</ul>
<h4>
	运行结果</h4>
<p class="info-box">
	Z:\PycharmProjects\book\ch18\18.3\spider_01_book&gt;scrapy crawl bookstoscrape -o bookstoscrape.csv<br />
	2020-10-14 14:14:52 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: spider_01_book)<br />
	2020-10-14 14:14:52 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.0 (v3.7.0:1bf9cc<br />
	5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h&nbsp; 22 Sep 2020), cryptography 3.1.1, Platform Windows-7-6.1.7601-SP1<br />
	2020-10-14 14:14:52 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor<br />
	2020-10-14 14:14:52 [scrapy.crawler] INFO: Overridden settings:<br />
	{&#39;BOT_NAME&#39;: &#39;spider_01_book&#39;,<br />
	&#39;NEWSPIDER_MODULE&#39;: &#39;spider_01_book.spiders&#39;,<br />
	&#39;ROBOTSTXT_OBEY&#39;: True,<br />
	&#39;SPIDER_MODULES&#39;: [&#39;spider_01_book.spiders&#39;]}<br />
	2020-10-14 14:14:52 [scrapy.extensions.telnet] INFO: Telnet Password: 304a52c2fceb08f2<br />
	2020-10-14 14:14:52 [scrapy.middleware] INFO: Enabled extensions:<br />
	[&#39;scrapy.extensions.corestats.CoreStats&#39;,<br />
	&#39;scrapy.extensions.telnet.TelnetConsole&#39;,<br />
	&#39;scrapy.extensions.feedexport.FeedExporter&#39;,<br />
	&#39;scrapy.extensions.logstats.LogStats&#39;]<br />
	2020-10-14 14:14:52 [scrapy.middleware] INFO: Enabled downloader middlewares:<br />
	[&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;,<br />
	&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;]<br />
	2020-10-14 14:14:52 [scrapy.middleware] INFO: Enabled spider middlewares:<br />
	[&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;,<br />
	&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;,<br />
	&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;,<br />
	&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;,<br />
	&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;]<br />
	2020-10-14 14:14:52 [scrapy.middleware] INFO: Enabled item pipelines:<br />
	[]<br />
	2020-10-14 14:14:52 [scrapy.core.engine] INFO: Spider opened<br />
	2020-10-14 14:14:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)<br />
	2020-10-14 14:14:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023<br />
	2020-10-14 14:14:53 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://books.toscrape.com/robots.txt&gt; (referer: None)<br />
	2020-10-14 14:14:53 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://books.toscrape.com/&gt; (referer: None)<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;A Light in the Attic&#39;, &#39;price&#39;: &#39;51.77&#39;, &#39;rate&#39;: &#39;Three&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;Tipping the Velvet&#39;, &#39;price&#39;: &#39;53.74&#39;, &#39;rate&#39;: &#39;One&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;Soumission&#39;, &#39;price&#39;: &#39;50.10&#39;, &#39;rate&#39;: &#39;One&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;Sharp Objects&#39;, &#39;price&#39;: &#39;47.82&#39;, &#39;rate&#39;: &#39;Four&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;Sapiens: A Brief History of Humankind&#39;, &#39;price&#39;: &#39;54.23&#39;, &#39;rate&#39;: &#39;Five&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;The Requiem Red&#39;, &#39;price&#39;: &#39;22.65&#39;, &#39;rate&#39;: &#39;One&#39;}<br />
	2020-10-14 14:14:53 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/&gt;<br />
	{&#39;name&#39;: &#39;The Dirty Little Secrets of Getting Your Dream Job&#39;, &#39;price&#39;: &#39;33.34&#39;, &#39;rate&#39;: &#39;Four&#39;}<br />
	... &lt;省略以下输出&gt; ...</p>
等待爬虫运行结束后，查看爬取到的数据，如图13所示。<br />
<br />
<div style="text-align: center;">
	<img alt="爬取到的数据" src="../uploads/allimg/230718/1-230GQ61914M3_2.png" /><br />
	图13：爬取到的数据</div>
<br />
从图13所示的数据可以看出，爬虫成功地爬取到了 1000 本书的书名和价格信息等（50 页，每页 20 项）。第一行是 3 个列名。<br />
<br />
这里导出的是 CSV 格式的文件，也可以导出 JSON 和 XML 格式的文件，代码如下所示。
<p class="info-box">
	scrapy crawl bookstoscrape-o bookstoscrape.jsonlines<br />
	scrapy crawl bookstoscrape-o bookstoscrape.xml</p>
</div>
	<div id="ggxc-weixin-arcbottom">
	<p>关注公众号「<span class="col-green">站长严长生</span>」，在手机上阅读所有教程，随时随地都能学习。内含一款搜索神器，免费下载全网书籍和视频。</p>
	<p style="margin-top:12px; text-align:center;">
		<img src="../templets/new/images/material/qrcode_mp_4.png" alt="公众号二维码" width="160" /><br />
		<span class="col-green">微信扫码关注公众号</span>
	</p>
</div>
	<div id="nice-arcs" class="box-bottom">
    <h4>推荐阅读</h4>
    <ul class="clearfix">
<li><a href="niz69i_8.html" title="一套完整的嵌入式开发学习路线（高薪就业版）" target="_blank">一套完整的嵌入式开发学习路线（高薪就业版）</a></li>
<li><a href="tnnfqo_4.html" title="一套课程卖1万，TMD太贵了！" target="_blank">一套课程卖1万，TMD太贵了！</a></li>
<li><a href="unnurw_4.html" title="跑了3000公里，见了一位大佬" target="_blank">跑了3000公里，见了一位大佬</a></li>
<li><a href="2293_2.html" title="Python MetaClass元类详解" target="_blank">Python MetaClass元类详解</a></li>
<li><a href="2425_2.html" title="MySQL DTAETIME、TIMESTAMP、DATE、TIME、YEAR（日期和时间类型）" target="_blank">MySQL DTAETIME、TIMESTAMP、DATE、TIME、YEAR（日期和时间类型）</a></li>
<li><a href="vip_3239_2.html" title="Shell函数返回值精讲" target="_blank">Shell函数返回值精讲</a></li>
<li><a href="4328_2.html" title="Python list列表详解" target="_blank">Python list列表详解</a></li>
<li><a href="6236_2.html" title="PHP instanceof：判断对象是否属于某个类" target="_blank">PHP instanceof：判断对象是否属于某个类</a></li>
<li><a href="../redis2/zremrangebyrank_2.html" title="Redis ZREMRANGEBYRANK命令" target="_blank">Redis ZREMRANGEBYRANK命令</a></li>
<li><a href="eb3j1zc_2.html" title="C# switch case语句详解" target="_blank">C# switch case语句详解</a></li>
</ul>
</div>
	
</div>
<script type="text/javascript">
// 当前文章ID
window.arcIdRaw = 'a_' + 10094;
window.arcId = "639c9xy/Sat3TUGwgdIGtM1nEX29F6rGgbnoHLd1ktD2PRpvBaVIDjdKyYRq";
window.typeidChain = "145|119";
</script>
<div id="footer" class="clearfix">
	<div class="info left">
	<p>精美而实用的网站，分享优质编程教程，帮助有志青年。千锤百炼，只为大作；精益求精，处处斟酌；这种教程，看一眼就倾心。</p>
	<p>
		<a href="8066_2.html" target="_blank" rel="nofollow">关于网站</a> <span>|</span>
		<a href="8092_3.html" target="_blank" rel="nofollow">关于站长</a> <span>|</span>
		<a href="8097_2.html" target="_blank" rel="nofollow">如何完成一部教程</a> <span>|</span>
		<a href="9648_2.html" target="_blank" rel="nofollow">公众号</a> <span>|</span>
		<a href="8093_2.html" target="_blank" rel="nofollow">联系我们</a> <span>|</span>
		<a href="../sitemap/sitemap_2.html" target="_blank" rel="nofollow">网站地图</a>
	</p>
	<p>Copyright ©2012-2022 biancheng.net, <a href="https://beian.miit.gov.cn" target="_blank" rel="nofollow" style="color:#666;">冀ICP备2022013920号</a>, <img height="13" src="../templets/new/images/gongan_2.png" alt="公安备案图标" /><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=13110202001352" target="_blank" rel="nofollow" style="color:#666;">冀公网安备13110202001352号</a>
	</p>
	</div>
	<img id="logo_bottom" class="right" src="https://m.biancheng.net/templets/new/images/logo_bottom_2.gif" alt="底部Logo" />
	<span id="return-top"><b>↑</b></span>
</div>

<div id="addweixin-widget">
	<p>
		<script type="text/javascript">
			/*var suffix = 'c';
			var thisMin = (new Date()).getMinutes();
			if(thisMin>=40){
				suffix = 'd';
			}else if(thisMin>=20){
				suffix = 'e';
			}else{
				suffix = 'c';
			}
			document.write('<img src="https://m.biancheng.net/templets/new/images/material/qrcode_wx_'%20+%20suffix%20+'.png?v=1.7.07" alt="微信交流群" width="120" /><br />');*/
		</script>
		<img src="https://m.biancheng.net/templets/new/images/material/qrcode_mp_4.png" alt="微信交流群" width="120" />
		<span>关注微信公众号，加入官方交流群。内含一款搜索神器，免费下载全网书籍和视频。</span>
	</p>
	<span id="close-addweixin-widget" class="iconfont iconfont-close"></span>
</div>

<script type="text/javascript">
window.siteId = 4;
window.cmsTemplets = "/templets/new";
window.cmsTempletsVer = "1.7.07";

</script>

<script src="https://m.biancheng.net/templets/new/script/jquery1.12.4.min_2.js"></script>
<script src="https://m.biancheng.net/templets/new/script/common_2.js"></script>
<!-- 51la V6 -->
<span style="display: none;">
<script charset="UTF-8" id="LA_COLLECT" src="https://sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id:"KDf6QzBhogyQjall",ck:"KDf6QzBhogyQjall",autoTrack:true})</script>
</span>
<!-- 51la V5 -->
<!-- <span style="display: none;"><script type="text/javascript" src="https://js.users.51.la/21368967.js"></script></span> -->
</body>
</html>